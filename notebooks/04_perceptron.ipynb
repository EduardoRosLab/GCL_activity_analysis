{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9edf848",
   "metadata": {},
   "source": [
    "# CLASSIFICATION OF THE MF AND GRC ACTIVITY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perceptron classification for the MF and GrC spiking activity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78824297",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### PARAMETERS #########################\n",
    "\n",
    "########## 01_anatomical_model_generation ################\n",
    "\n",
    "seed = 3 # seed for the generation of cells position\n",
    "\n",
    "########## 02_input_patterns_generation ##################\n",
    "\n",
    "## Spatial correlation between MFs \n",
    "sigma = 5 \n",
    "\n",
    "## List of MF active fractions that will be used \n",
    "f_mf = np.linspace(.05, 0.95, 10)\n",
    "\n",
    "## Number of patterns to be simulate for each\n",
    "## MF active fraction \n",
    "num_patterns = 640\n",
    "\n",
    "################ 03_simulations ##########################\n",
    "\n",
    "## Interval used to count spikes \n",
    "## during each pattern in the EDLUT simulation\n",
    "dt = interval = 0.070 \n",
    "\n",
    "## List of noises fractions to be tested\n",
    "noises = [0.2]\n",
    "\n",
    "## Total synaptic weight that reaches each GrC from MFs\n",
    "mf_grc_w = 4.00\n",
    "\n",
    "# Inhibitory weight in GoC to GrC synapsis\n",
    "goc_grc_w = 0.50 \n",
    "\n",
    "## List of synaptic weights to simulate from MF to GoC\n",
    "mf_goc_weights = [0.0, 0.10]\n",
    "\n",
    "## List of synaptic weights to simulate from GrC to GoC\n",
    "grc_goc_weights = [0.02]\n",
    "\n",
    "## Number of presentations/samples for each pattern\n",
    "## Important parameter when noise != 0.0\n",
    "n_seeds = 5 \n",
    "\n",
    "################ 04_perceptron ###########################\n",
    "\n",
    "## Perceptron seed, used when splitting the dataset\n",
    "## in training, validation and test\n",
    "perceptron_seed = seed\n",
    "\n",
    "## Number of classes for classification\n",
    "C = 10 \n",
    "\n",
    "## Backpropagation parameters\n",
    "gamma = 0.001 # learning rate of the perceptron\n",
    "batch_size = 32\n",
    "N_epochs = 500 #Number of epochs\n",
    "\n",
    "\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '../data/spikes')\n",
    "# Function to get MF spikes samples \n",
    "def get_samples_mf(fraction, noise, k):\n",
    "    \n",
    "    x_mf = np.loadtxt('../data/spikes/seed' + str(seed) \n",
    "                        + '/sigma_' + str(sigma) \n",
    "                        + '/noise_' + str(noise)\n",
    "                        + '/mf'\n",
    "                        + '/f_mf_' + str(fraction) \n",
    "                        + '/i' + str(interval) + '_mf_s' + str(k) + '.csv')\n",
    "\n",
    "    return x_mf\n",
    "\n",
    "# Function to get GrC spikes samples\n",
    "def get_samples_grc(fraction, noise, mf_goc_w, grc_goc_w, k):\n",
    "\n",
    "    x_grc = np.loadtxt('../data/spikes/seed' + str(seed) \n",
    "                        + '/sigma_' + str(sigma) \n",
    "                        + '/noise_' + str(noise)\n",
    "                        + '/grc'\n",
    "                        + '/mf_grc_' + str(mf_grc_w) \n",
    "                        + '/mf_goc_w_' + str(mf_goc_w)\n",
    "                        + '/goc_grc_w_' + str(goc_grc_w)\n",
    "                        + '/grc_goc_w_' + str(grc_goc_w)\n",
    "                        +'/f_mf_' + str(fraction) \n",
    "                        + '/i' + str(interval) + '_grc_s' + str(k) + '.csv')\n",
    "    \n",
    "    return x_grc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cbb744",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataframe columns names for saving the accuracies \n",
    "columns = [\"positions_seed\", \"perceptron_seed\", \"sigma\", \"fraction\", \n",
    "                            \"noise\", \"mf_grc_w\", \n",
    "                            \"mf_goc_w\", \"grc_goc_w\", \"goc_grc_w\", \"interval\", \"N_epochs\",\n",
    "                            \"C\", \"num_patterns\", \"n_seeds\", \"gamma\", \"batch_size\", \"type\", \"filename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c35746",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## For each noise fraction in noises list\n",
    "for noise in noises:\n",
    "\n",
    "    ## For each MF active fraction \n",
    "    for elem in f_mf:\n",
    "\n",
    "        fraction = elem\n",
    "        \n",
    "        ## Setting early stopping \n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "        ## Initializing seed\n",
    "        np.random.seed(seed)\n",
    "        keras.utils.set_random_seed(perceptron_seed)\n",
    "        \n",
    "        ########## MF perceptron classification ##############################################\n",
    "        \n",
    "        print(\"\")\n",
    "        print('Sigma: ' + str(sigma))\n",
    "        print('f_mf: ' + str(fraction))\n",
    "        print(\"\")\n",
    "    \n",
    "        ## Defining optimizer\n",
    "        opt = keras.optimizers.SGD(learning_rate=gamma)\n",
    "            \n",
    "        \n",
    "        # Defining the single layer of the perceptron \n",
    "        x = get_samples_mf(fraction, noise, 1) \n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(C, input_shape=(x.shape[0],),\n",
    "                            activation='softmax', kernel_initializer='random_uniform')]) \n",
    "\n",
    "        # Using accuracy as metric\n",
    "        model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        \n",
    "        ## Creating the datasets by joining the samples for all the presentations (n_seeds)\n",
    "        \n",
    "        ## Assigning each pattern to a class\n",
    "        y = np.zeros((C,num_patterns))\n",
    "        for k in range(num_patterns):\n",
    "            y[np.random.choice(C),k] = 1\n",
    "        y = np.tile(y, reps = n_seeds)\n",
    "\n",
    "        ## Joining datasets\n",
    "        x = get_samples_mf(fraction, noise, 0)\n",
    "        x = x[:,:num_patterns]\n",
    "        for i in range(n_seeds - 1): \n",
    "            x1 = get_samples_mf(fraction, i+1)\n",
    "            x1 = x1[:,:num_patterns]\n",
    "            x = np.concatenate((x, x1), axis = 1)\n",
    "           \n",
    "        \n",
    "        ## Splitting datasets in train, val and test (70%, 10% and 20%)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x.T, y.T, test_size=0.2, random_state=perceptron_seed, stratify=y.T)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.125, random_state=perceptron_seed, stratify=y_train)\n",
    "    \n",
    "        ## Fitting the model \n",
    "        history = model.fit(x_train, y_train, \n",
    "                            epochs=N_epochs, \n",
    "                            validation_data=(x_val, y_val), \n",
    "                            batch_size=batch_size, \n",
    "                            callbacks=[callback], verbose = 0)\n",
    "\n",
    "        ## Retrieving accuracy values\n",
    "        acc_train_mf = history.history['accuracy']\n",
    "        acc_val_mf = history.history['val_accuracy']\n",
    "        acc_test_mf = model.evaluate(x_test, y_test)[-1]\n",
    "\n",
    "        print('Accuracies:')\n",
    "        print('Training MF: ' + str(acc_train_mf[-1]))\n",
    "        print('Validation MF: ' + str(acc_val_mf[-1]))\n",
    "        print('Test MF: ' + str(acc_test_mf))\n",
    "\n",
    "        ########## GrC perceptron classification ##############################################\n",
    "        \n",
    "        ## For each MF - GoC weight\n",
    "        for mf_goc_w in mf_goc_weights:\n",
    "            \n",
    "            ## For each GrC - GoC weight\n",
    "            for grc_goc_w in grc_goc_weights: \n",
    "\n",
    "                print(\"\")\n",
    "                print('MF - GOC weight: ' + str(mf_goc_w))\n",
    "                print(\"\")\n",
    "                print('GRC - GOC weight: ' + str(grc_goc_w))\n",
    "                    \n",
    "                sys.path.insert(1, '../results')\n",
    "\n",
    "                 \n",
    "                # Defining optimizer\n",
    "                opt = keras.optimizers.SGD(learning_rate=gamma)\n",
    "\n",
    "                # Defining the single layer of the perceptron\n",
    "                x = get_samples_grc(fraction, noise, mf_goc_w, grc_goc_w, 1)\n",
    "                model = keras.Sequential([\n",
    "                    keras.layers.Dense(C, input_shape=(x.shape[0],),\n",
    "                                    activation='softmax', kernel_initializer='random_uniform')]) \n",
    "\n",
    "\n",
    "                # Using accuracy as metric\n",
    "                model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "                \n",
    "                ## Creating the datasets by joining the samples for all the presentation (n_seeds)\n",
    "                \n",
    "                ## Assigning each pattern to a class\n",
    "                y = np.zeros((C,num_patterns))\n",
    "                for k in range(num_patterns):\n",
    "                    y[np.random.choice(C),k] = 1\n",
    "                y = np.tile(y, reps = n_seeds)\n",
    "\n",
    "                ## Joining datasets\n",
    "                x = get_samples_grc(fraction, noise, mf_goc_w, grc_goc_w, 0)\n",
    "                x = x[:,:num_patterns]\n",
    "                for i in range(n_seeds - 1): \n",
    "                    x1 = get_samples_grc(fraction, noise, mf_goc_w, grc_goc_w, i+1)\n",
    "                    x1 = x1[:,:num_patterns]\n",
    "                    x = np.concatenate((x, x1), axis = 1)\n",
    "\n",
    "                    \n",
    "                ## Splitting datasets in train, val and test (70%, 10% and 20%)\n",
    "                x_train, x_test, y_train, y_test = train_test_split(x.T, y.T, test_size=0.2, random_state=perceptron_seed, stratify=y.T)\n",
    "                x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.125, random_state=perceptron_seed, stratify=y_train)\n",
    "\n",
    "                ## Fitting the model \n",
    "                history = model.fit(x_train, y_train, \n",
    "                                    epochs=N_epochs, \n",
    "                                    validation_data=(x_val, y_val), \n",
    "                                    batch_size=batch_size, \n",
    "                                    callbacks=[callback], verbose = 0)\n",
    "                               \n",
    "                ## Retrieving accuracy values\n",
    "                acc_train_grc = history.history['accuracy']\n",
    "                acc_val_grc = history.history['val_accuracy']\n",
    "                acc_test_grc = model.evaluate(x_test, y_test)[-1]\n",
    "\n",
    "\n",
    "                print('Accuracies:')\n",
    "                print('Training GRC: ' + str(acc_train_grc[-1]))\n",
    "                print('Validation GRC: ' + str(acc_val_grc[-1]))\n",
    "                print('Test GRC: ' + str(acc_test_grc))\n",
    "\n",
    "                acc_train = [acc_train_mf, acc_train_grc]\n",
    "                acc_val = [acc_val_mf, acc_val_grc]\n",
    "                acc_test = [acc_test_mf, acc_test_grc]\n",
    "               \n",
    "                \n",
    "                ## Creating the directory for accuracies\n",
    "                RESULTS_DIR = '../results/accuracies' + '/seed' + str(seed) \n",
    "                os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "                ## Retrieving actual time to save the results file\n",
    "                timenow  = datetime.datetime.now().strftime('%m_%d_%Y_%H_%M_%S')\n",
    "                \n",
    "                ## TRAIN RESULTS\n",
    "                acc_train = pd.DataFrame(acc_train, index = ['MF', 'GRC'])\n",
    "                acc_train.to_csv(RESULTS_DIR + '/train_' + str(timenow) + '.csv')\n",
    "\n",
    "                ## VALIDATION RESULTS\n",
    "                acc_val = pd.DataFrame(acc_val, index = ['MF', 'GRC'])\n",
    "                acc_val.to_csv(RESULTS_DIR + '/val_' + str(timenow) + '.csv')\n",
    "\n",
    "                ## TEST RESULTS\n",
    "                acc_test = pd.DataFrame(acc_test, index = ['MF', 'GRC'])\n",
    "                acc_test.to_csv(RESULTS_DIR + '/test_' + str(timenow) + '.csv')\n",
    "\n",
    "                ## Saving all the params information in a file\n",
    "                params_file_name = RESULTS_DIR + '/params_file.csv' \n",
    "\n",
    "                row_train = [seed, perceptron_seed, sigma, round(fraction, 2), noise, mf_grc_w, mf_goc_w, grc_goc_w, goc_grc_w, interval, N_epochs,\n",
    "                                    C, num_patterns, n_seeds, gamma, batch_size,\n",
    "                                    'train', 'train_' + str(timenow) + '.csv']\n",
    "                row_test = [seed, perceptron_seed, sigma, round(fraction, 2), noise, mf_grc_w, mf_goc_w, grc_goc_w, goc_grc_w, interval, N_epochs,\n",
    "                                    C, num_patterns, n_seeds, gamma, batch_size,\n",
    "                                    'test', 'test_' + str(timenow) + '.csv']\n",
    "                row_val = [seed, perceptron_seed, sigma, round(fraction, 2), noise, mf_grc_w, mf_goc_w, grc_goc_w, goc_grc_w, interval, N_epochs,\n",
    "                                    C, num_patterns, n_seeds, gamma, batch_size,\n",
    "                                    'val', 'val_' + str(timenow) + '.csv']\n",
    "\n",
    "                \n",
    "                # If dataframe does not yet exist\n",
    "                if not os.path.isfile(params_file_name):\n",
    "\n",
    "                    params_df = pd.DataFrame([row_train, row_test, row_val], columns = columns)\n",
    "                    params_df.to_csv(params_file_name, index = 0)\n",
    "\n",
    "                # If dataframe already exists\n",
    "                else:\n",
    "                    params_df = pd.read_csv(params_file_name)\n",
    "                    params_df = pd.concat([params_df, pd.DataFrame([row_train, row_test, row_val], columns = columns)])\n",
    "                    params_df.to_csv(params_file_name, index = 0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
